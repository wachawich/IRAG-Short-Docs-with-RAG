{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain\n",
    "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#Lib Extract PDF\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"<project>\"\n",
    "location = \"<location>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropicVertex(\n",
    "    model_name=\"claude-3-5-sonnet@20240620\",\n",
    "    project=project,\n",
    "    location=location,\n",
    "    temperature=0.1,\n",
    "    max_tokens=6046,\n",
    "    timeout=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_clean_text = \"\"\"\n",
    "    \n",
    "    Your task is to clean and restore the text extracted from a PDF document. \\n\n",
    "    The text may be poorly formatted, contain errors, or have inconsistent spacing and characters due to the conversion process. \\n\n",
    "    Your goal is to correct these issues and reconstruct the text so that it matches its original, \\n\n",
    "    intended form as closely as possible.\n",
    "  \n",
    "    DO NOT show \"Here's the cleaned and restored version of the document:\"\n",
    "    \n",
    "    this is my document: {raw_text}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "prompt_clean_text = PromptTemplate(template=template_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_keyword = \"\"\"\n",
    "    \n",
    "    Your task is to extract potential keywords from a given document. keyword always in \"Keyword ที่ user อาจใช้\" , \"ตัวอย่าง Error\"\n",
    "    \n",
    "    You should answer in Thai Languages\n",
    "    \n",
    "    You Should generate potential questions that a customer might ask based on the content of a given document. Think about the different ways a customer might phrase their inquiries to find relevant information. If the document contains examples, use them to inspire additional questions. The goal is to anticipate all possible questions a customer might ask about the document.\n",
    "    \n",
    "    You should answer in format list:\n",
    "    [\n",
    "    \"\n",
    "    Keyword:\n",
    "        - \n",
    "        - \n",
    "        - \n",
    "    \"\n",
    "    ,\n",
    "    \"\n",
    "    Error:\n",
    "        - \n",
    "        - \n",
    "        - \n",
    "    \"\n",
    "    ,\n",
    "    \"\n",
    "    Question:\n",
    "        -\n",
    "        -\n",
    "        -\n",
    "    \"\n",
    "    ]\n",
    "        \n",
    "    DO NOT show Other keyword\n",
    "\n",
    "    \n",
    "    this is my document: {clean_texts}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "prompt_keyword = PromptTemplate(template=template_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_clean_text = prompt_clean_text | llm\n",
    "chain_keyword = prompt_keyword | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import key_param as key_param\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = key_param.OPENAI_API_KEY\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = str(text)\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   \n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(text):\n",
    "    \"\"\"\n",
    "    เปลี่ยนจาก list ให้เป็น Text\n",
    "    \"\"\"\n",
    "    elements = text.strip(\"[]\").split(\",\")\n",
    "    return [element.strip().strip('\"') for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_PDF(folder_path, file_name):\n",
    "    \"\"\"\n",
    "    อ่านไฟล์ PDF เพื่อนำไปเป็น Text ปกติ\n",
    "    \"\"\"\n",
    "\n",
    "    path = folder_path + file_name\n",
    "    reader = PdfReader(str(path))\n",
    "    \n",
    "    raw_text = ''\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            raw_text += text\n",
    "            \n",
    "\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text, chain_clean_text=chain_clean_text):\n",
    "    \"\"\"\n",
    "    เปลี่ยน Raw Text เป็น Clean Text\n",
    "    \"\"\"\n",
    "\n",
    "    fianl_answer = chain_clean_text.invoke(\n",
    "        {\n",
    "            \"raw_text\": {raw_text},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return fianl_answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_keyword(clean_texts, chain_keyword=chain_keyword):\n",
    "    \"\"\"\n",
    "    นำ Clean Text มาทำเป็น Keyword\n",
    "    \"\"\"\n",
    "    fianl_answer = chain_keyword.invoke(\n",
    "        {\n",
    "            \"clean_texts\": {clean_texts},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    answer = fianl_answer.content\n",
    "\n",
    "    result = text_to_list(answer)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    อ่านไฟล์ PDF ทั้งหมด จาก Folder\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        return file_names\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder '{folder_path}' not found.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied to access '{folder_path}'.\")\n",
    "        return []\n",
    "\n",
    "folder_path = './PDF/'\n",
    "files = list_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CSV\n",
    "data = {\n",
    "    \"FileName\": [],\n",
    "    \"Content\": [],\n",
    "    \"Keyword\": [],\n",
    "    \"ada_embedding\": []\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"FileName\", \"Content\", \"Keyword\", \"ada_embedding\"]\n",
    "\n",
    "for file_name in tqdm(files):\n",
    "    \n",
    "    raw_text = read_PDF(folder_path, file_name)\n",
    "    \n",
    "    clean_texts = clean_text(raw_text)\n",
    "    \n",
    "    keyword = invoke_keyword(clean_texts)\n",
    "    \n",
    "    for row in range(len(keyword)):\n",
    "        \n",
    "        embedding = get_embedding(keyword[row])\n",
    "        \n",
    "        rows = [file_name, clean_texts, keyword[row], embedding]  \n",
    "        \n",
    "        new_df = pd.DataFrame([rows], columns=columns)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    time.sleep(10) # care timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"PDF_VectorStore.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
