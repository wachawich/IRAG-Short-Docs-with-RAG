{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import key_param\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = key_param.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = str(text)\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   \n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_csv_columns(file_list):\n",
    "    \"\"\"\n",
    "    Concat ระหว่าง PDF และ CSV โดยเอา column Content และ ada_embedding\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "        df = pd.read_csv(\"./\" + file, usecols=['Content', 'ada_embedding'])\n",
    "        \n",
    "\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    combined_df['ada_embedding'] = combined_df.ada_embedding.apply(eval).apply(np.array)\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['CSV_VectorStore.csv', 'PDF_VectorStore.csv']\n",
    "vector_stores = concatenate_csv_columns(file_list)\n",
    "\n",
    "vector_stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def search_in_docs(vector_stores, query, n=5, pprint=True):\n",
    "   \"\"\"\n",
    "    จะเป็นการนำ Query มา Embedding และ Search ใน vector_stores\n",
    "   \"\"\"\n",
    "\n",
    "   embedding = get_embedding(query, model='text-embedding-3-small')\n",
    "   query_embedding_2d = [embedding]\n",
    "   \n",
    "   vector_stores['similarities'] = vector_stores.ada_embedding.apply(lambda x: cosine_similarity([x], query_embedding_2d))\n",
    "   \n",
    "   k_search = vector_stores.sort_values('similarities', ascending=False).head(n)\n",
    "   \n",
    "   return k_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"วิธีการทำ RAG\"\n",
    "answer = search_in_docs(vector_stores, query, 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_from_query(df):\n",
    "    doc_list = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        embedding = row['ada_embedding']\n",
    "        document = Document(\n",
    "            page_content=row['Content'],\n",
    "            metadata={\n",
    "                # 'source': f'./{row['FileName']}',\n",
    "                'embedding': embedding\n",
    "            }\n",
    "        )\n",
    "        doc_list.append(document)\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_question(query,vector_store=vector_stores,  n=8):\n",
    "    \n",
    "    search = search_in_docs(vector_store, query, n)\n",
    "    \n",
    "    documents = create_document_from_query(search)\n",
    "    \n",
    "    return documents, search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"<project>\"\n",
    "location = \"<location>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropicVertex(\n",
    "    model_name=\"claude-3-5-sonnet@20240620\",\n",
    "    project=project,\n",
    "    location=location,\n",
    "    temperature=0.1,\n",
    "    max_tokens=6046,\n",
    "    timeout=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    Your AI is named Wachi , and ...... (Assitant Prompt)\n",
    "    \n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"query\"], template=template)\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(query, chain=chain):\n",
    "    \n",
    "    documents, df = documents_question(query)\n",
    "\n",
    "    inputs = {\n",
    "        'input_documents': documents,\n",
    "        'query': query,\n",
    "    }\n",
    "\n",
    "    answer = chain.run(**inputs)\n",
    "    \n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"วิธีการทำ RAG\"\n",
    "print(QA(query))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
